{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "! make clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "! make clean-logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm  -rf /media/sayem/510B93E12554BBD1/Hangman/wandb\n",
    "# ! rm -rf /media/sayem/510B93E12554BBD1/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "DATASET_DIR = PROJECT_ROOT / \"dataset\"\n",
    "\n",
    "from dataset.data_generation import read_words_list\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # Numpy module\n",
    "    torch.manual_seed(seed)  # PyTorch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # Sets seed for CUDA (GPU)\n",
    "        torch.cuda.manual_seed_all(seed)  # Ensure reproducibility on all GPUs\n",
    "        torch.backends.cudnn.deterministic = True  # Use deterministic algorithms\n",
    "        torch.backends.cudnn.benchmark = (\n",
    "            False  # If input sizes do not vary, this should be set to False\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "set_seed(42)  # Use any number to seed all libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = DATA_DIR / \"words_250000_train.txt\"\n",
    "corpus = read_words_list(corpus_path)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.hangman_api import HangmanAPI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "env_path = PROJECT_ROOT / \".env\"\n",
    "api_key = None\n",
    "if env_path.exists():\n",
    "    with env_path.open() as env_file:\n",
    "        for raw_line in env_file:\n",
    "            line = raw_line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            key, _, value = line.partition(\"=\")\n",
    "            if key.strip() == \"API\":\n",
    "                value = value.strip()\n",
    "                value = value.strip('\"')\n",
    "                value = value.strip(\"'\")\n",
    "                api_key = value\n",
    "                break\n",
    "\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"API key not found in .env\")\n",
    "\n",
    "os.environ[\"API\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "from api.guess_strategies import (\n",
    "    neural_guess_strategy,\n",
    "    frequency_guess_strategy,\n",
    "    ngram_guess_strategy,\n",
    ")\n",
    "from dataset.encoder_utils import DEFAULT_ALPHABET\n",
    "from models import (\n",
    "    HangmanBiLSTM,\n",
    "    HangmanBiLSTMConfig,\n",
    "    HangmanLightningModule,\n",
    "    HangmanTransformer,\n",
    "    HangmanTransformerConfig,\n",
    "    TrainingModuleConfig,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"hangman.notebook\")\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def load_neural_strategy(checkpoint_dir: Path) -> tuple[callable, str]:\n",
    "    checkpoint_files = sorted(checkpoint_dir.glob(\"best-hangman-*.ckpt\"))\n",
    "    if not checkpoint_files:\n",
    "        logger.warning(\n",
    "            \"No checkpoints found in %s. Falling back to frequency strategy.\",\n",
    "            checkpoint_dir,\n",
    "        )\n",
    "        return frequency_guess_strategy, \"frequency\"\n",
    "\n",
    "    def _score(path: Path) -> float:\n",
    "        try:\n",
    "            return float(path.stem.split(\"=\")[-1])\n",
    "        except ValueError:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "    best_checkpoint = max(checkpoint_files, key=_score)\n",
    "    logger.info(\"Loading checkpoint: %s\", best_checkpoint.name)\n",
    "\n",
    "    checkpoint = torch.load(best_checkpoint, map_location=\"cpu\")\n",
    "\n",
    "    model_class_name = \"HangmanBiLSTM\"\n",
    "    hyper_params = checkpoint.get(\"hyper_parameters\")\n",
    "    if isinstance(hyper_params, dict):\n",
    "        model_class_name = hyper_params.get(\"model_class\", model_class_name)\n",
    "\n",
    "    vocab_size = len(DEFAULT_ALPHABET)\n",
    "    mask_idx = len(DEFAULT_ALPHABET)\n",
    "    pad_idx = len(DEFAULT_ALPHABET) + 1\n",
    "\n",
    "    if \"Transformer\" in model_class_name:\n",
    "        model_config = HangmanTransformerConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            mask_idx=mask_idx,\n",
    "            pad_idx=pad_idx,\n",
    "            max_word_length=45,\n",
    "        )\n",
    "        base_model = HangmanTransformer(model_config)\n",
    "    else:\n",
    "        model_config = HangmanBiLSTMConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            mask_idx=mask_idx,\n",
    "            pad_idx=pad_idx,\n",
    "        )\n",
    "        base_model = HangmanBiLSTM(model_config)\n",
    "\n",
    "    lightning_module = HangmanLightningModule(base_model, TrainingModuleConfig())\n",
    "    lightning_module.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    lightning_module.eval()\n",
    "\n",
    "    return (\n",
    "        partial(neural_guess_strategy, model=lightning_module.model),\n",
    "        best_checkpoint.name,\n",
    "    )\n",
    "\n",
    "\n",
    "dictionary_path = (DATA_DIR / \"words_250000_train.txt\").resolve()\n",
    "checkpoint_dir = (PROJECT_ROOT / \"logs\" / \"checkpoints\").resolve()\n",
    "\n",
    "# try:\n",
    "strategy_callable, checkpoint_name = load_neural_strategy(checkpoint_dir)\n",
    "# if checkpoint_name != \"frequency\":\n",
    "#     logger.info(\"Using neural strategy from %s\", checkpoint_name)\n",
    "# else:\n",
    "#     logger.info(\"Using fallback frequency strategy\")\n",
    "# except Exception as exc:  # noqa: BLE001 - display to notebook\n",
    "# logger.exception(\n",
    "#     \"Failed to load neural strategy, falling back to frequency strategy\"\n",
    "# )\n",
    "# strategy_callable = frequency_guess_strategy\n",
    "\n",
    "api = HangmanAPI(\n",
    "    access_token=api_key,\n",
    "    dict_path=str(dictionary_path),\n",
    "    strategy=strategy_callable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a single game\n",
    "api.start_game(practice=1, verbose=True)\n",
    "[\n",
    "    total_practice_runs,\n",
    "    total_recorded_runs,\n",
    "    total_recorded_successes,\n",
    "    total_practice_successes,\n",
    "] = api.my_status()  # Get my game stats: (# of tries, # of wins)\n",
    "practice_success_rate = total_practice_successes / total_practice_runs\n",
    "print(\n",
    "    \"run %d practice games out of an allotted 100,000. practice success rate so far = %.3f\"\n",
    "    % (total_practice_runs, practice_success_rate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status before practice run\n",
    "before_status = api.my_status()\n",
    "(\n",
    "    before_practice_runs,\n",
    "    before_recorded_runs,\n",
    "    before_recorded_successes,\n",
    "    before_practice_successes,\n",
    ") = before_status\n",
    "\n",
    "before_practice_win_rate = (\n",
    "    before_practice_successes / before_practice_runs if before_practice_runs else 0.0\n",
    ")\n",
    "\n",
    "print(f\"Before practice runs: {before_practice_runs}\")\n",
    "print(f\"Before practice successes: {before_practice_successes}\")\n",
    "print(f\"Before recorded runs: {before_recorded_runs}\")\n",
    "print(f\"Before recorded successes: {before_recorded_successes}\")\n",
    "print(f\"Practice win rate so far: {before_practice_win_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run 1000 practice games\n",
    "from tqdm import tqdm\n",
    "\n",
    "practice_games = 1_0\n",
    "for game_index in tqdm(range(practice_games), desc=\"Practice games\", unit=\"game\"):\n",
    "    api.start_game(practice=1, verbose=False)\n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 1000 practice games\n",
    "after_status = api.my_status()\n",
    "(\n",
    "    after_practice_runs,\n",
    "    after_recorded_runs,\n",
    "    after_recorded_successes,\n",
    "    after_practice_successes,\n",
    ") = after_status\n",
    "\n",
    "after_practice_win_rate = (\n",
    "    after_practice_successes / after_practice_runs if after_practice_runs else 0.0\n",
    ")\n",
    "\n",
    "print(f\"After practice runs: {after_practice_runs}\")\n",
    "print(f\"After practice successes: {after_practice_successes}\")\n",
    "print(f\"After recorded runs: {after_recorded_runs}\")\n",
    "print(f\"After recorded successes: {after_recorded_successes}\")\n",
    "print(f\"Practice win rate so far: {after_practice_win_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session performance\n",
    "practice_runs_delta = after_practice_runs - before_practice_runs\n",
    "practice_successes_delta = after_practice_successes - before_practice_successes\n",
    "recorded_runs_delta = after_recorded_runs - before_recorded_runs\n",
    "recorded_successes_delta = after_recorded_successes - before_recorded_successes\n",
    "\n",
    "session_practice_win_rate = (\n",
    "    practice_successes_delta / practice_runs_delta\n",
    "    if practice_runs_delta\n",
    "    else float(\"nan\")\n",
    ")\n",
    "print(\"Session performance:\")\n",
    "print(f\"Δ practice runs: {practice_runs_delta}\")\n",
    "print(f\"Δ practice successes: {practice_successes_delta}\")\n",
    "print(f\"Δ recorded runs: {recorded_runs_delta}\")\n",
    "print(f\"Δ recorded successes: {recorded_successes_delta}\")\n",
    "if practice_runs_delta:\n",
    "    print(f\"Session practice win rate: {session_practice_win_rate:.2%}\")\n",
    "else:\n",
    "    print(\"Session practice win rate: n/a (no new practice games)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session performance summary computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run 1000 practice games\n",
    "from tqdm import tqdm\n",
    "\n",
    "practice_games = 1_000\n",
    "for game_index in tqdm(range(practice_games), desc=\"Practice games\", unit=\"game\"):\n",
    "    # api.start_game(practice=0, verbose=False)\n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    total_practice_runs,\n",
    "    total_recorded_runs,\n",
    "    total_recorded_successes,\n",
    "    total_practice_successes,\n",
    "] = api.my_status()  # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes / total_recorded_runs\n",
    "print(\"overall success rate = %.3f\" % success_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orchestra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
