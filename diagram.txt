# Current flow:
Input: [batch, seq_len] of token IDs
   ↓
Embedding
   ↓
BiLSTM
   ↓
Linear layer
   ↓
Output: [batch, seq_len, vocab_size] logits

# Experiment with resoaning:
Input: [batch, seq_len] of token IDs
   ↓
Embedding
   ↓
BiLSTM
   ↓
   ├─────────────┬─────────────┐
   ↓             ↓             ↓
BiLSTM out   BigramRule   PositionRule  ← NEW RULES
   ↓             ↓             ↓
   └─────────────┴─────────────┘
               ↓
          Combine scores
               ↓
Output: [batch, seq_len, vocab_size] logits

### Code
from __future__ import annotations
from dataclasses import dataclass
import logging
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from models.architectures.base import BaseArchitecture, BaseConfig


@dataclass
class HangmanBiLSTMConfig(BaseConfig):
    embedding_dim: int = 256
    hidden_dim: int = 256
    num_layers: int = 4
    dropout: float = 0.3
    use_rules: bool = True  # NEW: enable rule-based reasoning


logger = logging.getLogger(__name__)


# NEW: Bigram Rule Module
class BigramRule(nn.Module):
    """Learn bigram patterns: what letter follows/precedes another."""
    
    def __init__(self, vocab_size: int):
        super().__init__()
        # Learnable matrix [vocab_size, vocab_size]
        # weights[i][j] = score for letter j appearing after letter i
        self.weights = nn.Parameter(torch.randn(vocab_size, vocab_size) * 0.01)
        
    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: [batch, seq_len] - token IDs
        Returns:
            scores: [batch, seq_len, vocab_size] - bigram scores
        """
        batch_size, seq_len = inputs.shape
        vocab_size = self.weights.size(0)
        
        scores = torch.zeros(batch_size, seq_len, vocab_size, device=inputs.device)
        
        # For each position, look at neighbors
        for pos in range(seq_len):
            # Look at previous letter (if exists)
            if pos > 0:
                prev_letter = inputs[:, pos - 1]  # [batch]
                # Get scores for what comes after prev_letter
                # weights[prev_letter, :] = scores for all possible next letters
                prev_scores = self.weights[prev_letter]  # [batch, vocab_size]
                scores[:, pos, :] += prev_scores
            
            # Look at next letter (if exists)
            if pos < seq_len - 1:
                next_letter = inputs[:, pos + 1]  # [batch]
                # Get scores for what comes before next_letter
                # weights[:, next_letter] = scores for all possible prev letters
                next_scores = self.weights[:, next_letter].t()  # [batch, vocab_size]
                scores[:, pos, :] += next_scores
        
        return scores


# NEW: Position Rule Module
class PositionRule(nn.Module):
    """Learn positional patterns: what letter is likely at each position."""
    
    def __init__(self, max_len: int, vocab_size: int):
        super().__init__()
        # Learnable matrix [max_len, vocab_size]
        # weights[pos][letter] = score for letter at position pos
        self.weights = nn.Parameter(torch.randn(max_len, vocab_size) * 0.01)
        
    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: [batch, seq_len] - token IDs (not used, but kept for interface)
        Returns:
            scores: [batch, seq_len, vocab_size] - position scores
        """
        batch_size, seq_len = inputs.shape
        
        # Just return position weights for each position
        # Expand to batch dimension
        scores = self.weights[:seq_len].unsqueeze(0).expand(batch_size, -1, -1)
        
        return scores


# MODIFIED: Main model with rules
class HangmanBiLSTM(BaseArchitecture):
    def __init__(self, config: HangmanBiLSTMConfig):
        super().__init__(config)

        vocab_with_special = config.get_vocab_size_with_special()
        self.embedding = nn.Embedding(
            num_embeddings=vocab_with_special,
            embedding_dim=config.embedding_dim,
            padding_idx=config.pad_idx,
        )

        self.dropout = nn.Dropout(config.dropout)
        self.lstm = nn.LSTM(
            input_size=config.embedding_dim,
            hidden_size=config.hidden_dim,
            num_layers=config.num_layers,
            batch_first=True,
            dropout=config.dropout if config.num_layers > 1 else 0.0,
            bidirectional=True,
        )

        # Original LSTM output layer
        self.lstm_output = nn.Linear(config.hidden_dim * 2, config.vocab_size)
        
        # NEW: Add rule modules
        if config.use_rules:
            self.bigram_rule = BigramRule(config.vocab_size)
            self.position_rule = PositionRule(
                max_len=100,  # adjust to your max word length
                vocab_size=config.vocab_size
            )
            
            # Learnable weights for combining different sources
            self.combine_weights = nn.Parameter(torch.ones(3))  # [lstm, bigram, position]

    def forward(self, inputs: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:
        logger.debug(
            "Forward called with inputs shape=%s, lengths shape=%s",
            tuple(inputs.shape),
            tuple(lengths.shape),
        )

        # Original BiLSTM forward pass
        embed = self.embedding(inputs)
        embed = self.dropout(embed)

        packed = pack_padded_sequence(
            embed, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        packed_output, _ = self.lstm(packed)
        lstm_output, _ = pad_packed_sequence(
            packed_output, batch_first=True, total_length=inputs.size(1)
        )

        # Get LSTM scores
        lstm_logits = self.lstm_output(self.dropout(lstm_output))
        
        # NEW: Add rule scores if enabled
        if hasattr(self, 'bigram_rule'):
            # Get rule scores
            bigram_scores = self.bigram_rule(inputs)
            position_scores = self.position_rule(inputs)
            
            # Normalize combination weights
            weights = torch.softmax(self.combine_weights, dim=0)
            
            # Combine: weighted sum of all scores
            logits = (
                weights[0] * lstm_logits +
                weights[1] * bigram_scores +
                weights[2] * position_scores
            )
        else:
            logits = lstm_logits

        logger.debug("Logits shape=%s", tuple(logits.shape))
        return logits